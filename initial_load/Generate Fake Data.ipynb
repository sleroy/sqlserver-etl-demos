{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%additional_python_modules Faker\n%idle_timeout 2880\n%glue_version 4.0\n%worker_type G.8X\n%number_of_workers 5\n%region us-east-1\n%connections sqljdbc\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.sql.types import StructType, StructField, BooleanType, BinaryType, IntegerType, StringType\n\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.5 \nAdditional python modules to be included:\nFaker\nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 4.0\nPrevious worker type: None\nSetting new worker type to: G.8X\nPrevious number of workers: None\nSetting new number of workers to: 5\nPrevious region: us-east-1\nSetting new region to: us-east-1\nRegion is set to: us-east-1\nConnections to be included:\nsqljdbc\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.8X\nNumber of Workers: 5\nIdle Timeout: 2880\nSession ID: be168f4e-3f21-4e7c-92f8-5597130b5c56\nApplying the following default arguments:\n--glue_kernel_version 1.0.5\n--enable-glue-datacatalog true\n--additional-python-modules Faker\nWaiting for session be168f4e-3f21-4e7c-92f8-5597130b5c56 to get into ready status...\nSession be168f4e-3f21-4e7c-92f8-5597130b5c56 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import logging\nlogger = logging.getLogger(__name__)\nlogger.info(\"Schema creation\")\n\n#Defining the schema of the dataframe.\nschema = StructType([\n    StructField(\"CustSurr\", BinaryType(), False),\n    StructField(\"IpId\", StringType(), False),\n    StructField(\"CustIntId\", StringType(), False),\n    StructField(\"LoadStatusId\", IntegerType(), False),\n    StructField(\"CustSurrInt\", IntegerType(), False),\n    StructField(\"IsAnonymized\", BooleanType(), False),    \n])\n\n\n\nfrom faker import Faker\nfrom faker.providers import internet\nfrom pyspark.sql import Row\nimport os\nimport struct\nfake = Faker()\nfake.add_provider(internet)\n\nmax_rows = 47000000\nnumber_occ = 1000000\n\n\n    # 47000000\nlogger.info(\"Creation of the fake data %d occurences\", number_occ)\nvals = []\nfor i in range(0, number_occ):\n     # Generate a random byte sequence\n    byte_sequence = os.urandom(16)\n\n    # Convert the byte sequence to a format suitable for PySpark\n    byte_sequence_hex = byte_sequence.hex()\n    byte_sequence_struct = struct.pack('16s', byte_sequence)\n    ar = (byte_sequence_struct, fake.ipv4_private(), fake.md5(), fake.pyint(), fake.pyint(),  fake.pybool() )\n    vals.append(ar) \nlogger.info(\"Data generated\")\n\n\nprint(\"Begin\")\n\nmaxTrials=round(max_rows/ number_occ) + 1\nfor trial in range(0, maxTrials):\n\n    logger.info(\"Sending thefake data %d/%d\", trial, maxTrials)\n    logger.info(\"Data Creation of the dataframe\")\n\n    fake_table = spark.createDataFrame(vals, schema)\n\n    logger.info(\"Creation of the dynamic frame\")\n    df = fake_table.toDF('CustSurr', 'IpId', 'CustIntId', 'LoadStatusId', 'CustSurrInt', 'IsAnonymized')\n\n    logger.info(\"Data generated\")\n    from awsglue.dynamicframe import DynamicFrame\n    dynamic_frame_write = DynamicFrame.fromDF(fake_table, glueContext, \"dynamic_frame_write\")\n    #logger.info(\"Write in S3\")\n    #glueContext.write_dynamic_frame.from_options(\n    #    frame = df    \n    #    connection_type = \"s3\",\n    #    format=\"parquet\",\n    #    connection_options = {\n    #        \"useGlueParquetWriter\": True,\n    #        \"path\": \"s3://ml-bucket-131313/danske/parquet\"\n    #    }\n    #)\n    logger.info(\"Write in SQL Server\")\n    # Script generated for node Microsoft SQL Server        \n    glueContext.write_dynamic_frame.from_jdbc_conf(dynamic_frame_write, \"sqljdbc\", connection_options={\"dbtable\": \"large_table\", \"database\": \"wine\"})\n    logger.info(\"Write finished\")\n",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Begin\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "job.commit()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 49,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		}
	]
}